spark-conf = {
  app-name = "lsa"
  master = "local[*]"
}

transcripts-file-path = "/root/apps/input/transcripts.tsv"

adhoc-file-path = "/root/apps/input/adhoc-parsed.tsv"

faqs-file-path = "/root/apps/input/faqs-parsed.tsv"

stopwords-file-path = "/root/apps/lsa/src/main/resources/stopwords.txt"

output-save-path = "lsa-out.csv"

nlp-annotators = ["tokenize" , "cleanxml", "ssplit", "pos", "lemma"]

svd = {
  num-concepts = 100
}

stats = {
  num-concepts = 10
  num-terms = 10
}

tfidf = {
  vocab-size = 2000
}

lda = {
  topics = 2
  iterations = 100
  vocab-size = 1000
}
